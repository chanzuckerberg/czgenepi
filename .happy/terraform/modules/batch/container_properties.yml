# This is a configuration of AWS Batch container overrides, used by the Terraform resources in this directory.
# See https://docs.aws.amazon.com/batch/latest/APIReference/API_ContainerOverrides.html for the specification.
# The definition source is in YAML for brevity and readability. It is compiled into JSON by Terraform
# then deployed by Terraform.
#
# Strings of the form ${aws_region} are interpreted by Terraform as variables. Use $${aws_region} to escape if that is not the intent.

image: "${batch_docker_image}"
command:
  - "/bin/bash"
  - "-c"
  - 'for i in "$@"; do eval "$i"; done; cd /'
  - "swipe"
  - "set -a"
  - "if [ -f /etc/environment ]; then source /etc/environment; fi"
  - "if [ -f /etc/default/locale ]; then source /etc/default/locale; else export LC_ALL=C.UTF-8 LANG=C.UTF-8; fi"
  - "set +a"
  - >-
    while true; do
    if TOKEN=`curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"` && curl -H "X-aws-ec2-metadata-token: $TOKEN" -sf http://169.254.169.254/latest/meta-data/spot/instance-action; then
    echo WARNING: THIS SPOT INSTANCE HAS BEEN SCHEDULED FOR TERMINATION >> /dev/stderr;
    fi;
    sleep 10;
    done &
  - >-
    put_metric() {
    aws cloudwatch put-metric-data --metric-name $1 --namespace swipe-$DEPLOYMENT_ENVIRONMENT --unit Percent --value $2 --dimensions SFNCurrentState=$SFN_CURRENT_STATE;
    }
  - >-
    while true; do
    put_metric ScratchSpaceInUse $(df --output=pcent /mnt | tail -n 1 | cut -f 1 -d %);
    put_metric CPULoad $(cat /proc/loadavg | cut -f 1 -d ' ' | cut -f 2 -d .);
    put_metric MemoryInUse $(python3 -c 'import psutil; m=psutil.virtual_memory(); print(100*(1-m.available/m.total))');
    sleep 60;
    done &
  - "mkdir -p /mnt/download_cache; touch /mnt/download_cache/_miniwdl_flock"
  - >-
    clean_wd() {
    (shopt -s nullglob;
    for wf_log in /mnt/20??????_??????_*/workflow.log; do
    flock -n $wf_log rm -rf $(dirname $wf_log) || true;
    done;
    flock -x /mnt/download_cache/_miniwdl_flock clean_download_cache.sh /mnt/download_cache $DOWNLOAD_CACHE_MAX_GB)
    }
  - "clean_wd"
  - "df -h / /mnt"
  - 'export MINIWDL__S3_PROGRESSIVE_UPLOAD__URI_PREFIX=$(dirname "$WDL_OUTPUT_URI")'
  - "if [ -f /etc/profile ]; then source /etc/profile; fi"
  - "miniwdl --version"
  - "set -euo pipefail"
  - 'export CURRENT_STATE=$(echo "$SFN_CURRENT_STATE" | sed -e s/SPOT// -e s/EC2//)'
  - 'aws s3 cp "$WDL_WORKFLOW_URI" .'
  - 'aws s3 cp "$WDL_INPUT_URI" wdl_input.json'
  - >-
    handle_error() {
    OF=wdl_output.json;
    EP=.cause.stderr_file;
    if jq -re .error $OF; then
    if jq -re $EP $OF; then
    if tail -n 1 $(jq -r $EP $OF) | jq -re .wdl_error_message; then
    tail -n 1 $(jq -r $EP $OF) > $OF;
    fi;
    fi;
    aws s3 cp $OF "$WDL_OUTPUT_URI";
    fi
    }
  - "trap handle_error EXIT"
  - 'miniwdl run --dir /mnt $(basename "$WDL_WORKFLOW_URI") --input wdl_input.json --verbose --error-json -o wdl_output.json'
  - "clean_wd"
environment:
  - name: "AWS_DEFAULT_REGION"
    value: "${aws_region}"
  - name: "DEPLOYMENT_STAGE"
    value: "${deployment_stage}"
  - name: "REMOTE_DEV_PREFIX"
    value: "${remote_dev_prefix}"
  - name: "FRONTEND_URL"
    value: "${frontend_url}"
  - name: "ASPEN_CONFIG_SECRET_NAME"
    value: "${deployment_stage}/aspen-config"
  - name: "WDL_INPUT_URI"
    value: "Set this variable to the S3 URI of the WDL input JSON"
  - name: "WDL_WORKFLOW_URI"
    value: "Set this variable to the S3 URI of the WDL workflow"
  - name: "WDL_OUTPUT_URI"
    value: "Set this variable to the S3 URI where the WDL output JSON will be written"
  - name: "SFN_EXECUTION_ID"
    value: "Set this variable to the current step function execution ARN"
  - name: "SFN_CURRENT_STATE"
    value: "Set this variable to the current step function state name, like HostFilterEC2 or HostFilterSPOT"
  - name: "DEPLOYMENT_ENVIRONMENT"
    value: "${deployment_environment}"
  - name: "AWS_DEFAULT_REGION"
    value: "${aws_region}"
  - name: "MINIWDL__S3PARCP__DOCKER_IMAGE"
    value: "${batch_docker_image}"
  - name: "MINIWDL__DOWNLOAD_CACHE__PUT"
    value: "true"
  - name: "MINIWDL__DOWNLOAD_CACHE__GET"
    value: "true"
  - name: "MINIWDL__DOWNLOAD_CACHE__DIR"
    value: /mnt/download_cache
  - name: "MINIWDL__DOWNLOAD_CACHE__DISABLE_PATTERNS"
    value: '["s3://swipe-samples-*/*"]'
  - name: "DOWNLOAD_CACHE_MAX_GB"
    value: "500"
jobRoleArn: "${batch_job_role_arn}"
volumes:
  - name: "scratch"
    host:
      sourcePath: "/mnt"
  - name: "docker_sock"
    host:
      sourcePath: "/var/run/docker.sock"
mountPoints:
  - sourceVolume: "scratch"
    containerPath: "/mnt"
    readOnly: false
  - sourceVolume: "docker_sock"
    containerPath: "/var/run/docker.sock"
    readOnly: false
ulimits:
  - name: "nofile"
    hardLimit: 100000
    softLimit: 100000
privileged: false
readonlyRootFilesystem: false
logConfiguration:
  logDriver: "awslogs"
  options:
    awslogs-group: "${log_group}"
    awslogs-region: "${aws_region}"

# The AWS Batch API requires two resource quotas: vCPU and memory. Memory contention or starvation is more dangerous
# than CPU contention (an OOM condition will cause a job to fail, while lower than expected CPU will just cause it to
# run longer). The Batch scheduler uses both quotas to schedule (pack) jobs onto instances, but only enforces the memory
# quota as a hard limit. We set both quotas to a token value here. The step function overrides the memory quota at
# runtime, causing it to become the concurrency-limiting factor for job packing. (Additional logic is required in
# miniwdl to set the memory hard limits on child containers running WDL tasks.)
vcpus: 1
memory: 1024
